# ğŸ² DSCI 551 â€“ Lecture 1 Notes : Depicting Uncertainty

## 1ï¸âƒ£ Core Concepts of Probability
- **Experiment** â†’ an action with uncertain outcome (coin toss, die roll).  
- **Sample space (S)** â†’ all possible outcomes.  
  - Example: `S = {Heads, Tails}`  
- **Event (E)** â†’ subset of S (e.g. `E = {Heads}`).

**Empirical probability**
\[
P(E) = \frac{\text{# times E occurs}}{\text{# total experiments}}
\]
approaches the true value as n â†’ âˆ.

---

## 2ï¸âƒ£ Axioms & Laws
1. **Axiom 1 (Sample space)** â†’ `P(S)=1`  
2. **Axiom 2 (Mutually exclusive events)** â†’ `P(AâˆªB)=P(A)+P(B)` if Aâˆ©B=âˆ…  
3. **Complement rule** â†’ `P(Ac)=1âˆ’P(A)`  
4. **Law of Total Probability** â†’ probability of E = sum of probabilities of its disjoint parts  
5. **Inclusionâ€“Exclusion Principle**


P(A âˆª B) = P(A) + P(B) âˆ’ P(A âˆ© B)

extended to 3+ events by alternating + / â€“ terms.  
6. **Independence** â†’ `P(A âˆ© B)=P(A)Â·P(B)`

---

## 3ï¸âƒ£ Odds â†” Probability
\[
o = \frac{p}{1-p}, \quad p = \frac{o}{o+1}
\]
Example: p = 0.8 â‡’ o = 4 : 1 (4 wins per loss)

---

## 4ï¸âƒ£ Random Variables (RV)
- **Discrete RV:** countable outcomes â†’ Probability Mass Function (PMF) `P(X=x)`
- **Continuous RV:** uncountable outcomes â†’ Probability Density Function (PDF) `fX(x)`
- Example: X = # heads in 10 coin tosses.

---

## 5ï¸âƒ£ Measures of Central Tendency & Uncertainty

| Measure | Meaning | Formula |
|----------|----------|----------|
| **Mean (E[X])** | expected/typical value | `Î£ x P(X=x)` or `âˆ« x f(x) dx` |
| **Variance (Var[X])** | spread/uncertainty | `E[(Xâˆ’E[X])Â²] = E[XÂ²]âˆ’E[X]Â²` |
| **Std Dev (Ïƒ)** | sqrt of variance | `Ïƒ = âˆšVar[X]` |
| **Mode** | most probable outcome | argmax P(X=x) |
| **Entropy H(X)** | randomness in bits/nats | `âˆ’Î£ P(X=x) log P(X=x)` |

---

## 6ï¸âƒ£ Examples to Remember
- **Mario Kart items** as a discrete distribution: probs sum to 1.  
- **Complement example:** P(not Coin) = 1 âˆ’ 0.75 = 0.25.  
- **Inclusionâ€“Exclusion example:** Explosion âˆ¨ Blue-shell item = 0.08.  
- **Entropy example:** H â‰ˆ 0.87 â†’ some randomness but not uniform.

---

## 7ï¸âƒ£ Key Formulas Summary
```text
P(Ac) = 1 âˆ’ P(A)
P(A âˆª B) = P(A) + P(B) âˆ’ P(A âˆ© B)
P(A âˆ© B) = P(A)Â·P(B)   # if independent
Var(X) = E[XÂ²] âˆ’ (E[X])Â²
H(X) = âˆ’Î£ P(x)Â·log P(x)
```
---

# ğŸ¯ DSCI 551 Lecture 2 â€“ Parametric Families

Refresher on probability distributions, random-variable transformations, and the most common discrete distribution families.

---

## 1ï¸âƒ£ Review â€” Properties of Distributions
Example PMF (Number of crabs per nest):

| x | P(X = x) |
|---|-----------|
| 0 | 0.4 |
| 1 | 0.1 |
| 2 | 0.1 |
| 3 | 0.4 |

**Mean**

\[
E[X] = \sum x P(X=x) = 1.5
\]

**Variance**

\[
Var(X) = E[(Xâˆ’E[X])Â²] = 1.85
\]

**Mode** â†’ values with max probability â†’ 0 and 3  
**Entropy**

\[
H(X) = âˆ’ \sum P(X=x) log P(X=x) = 1.19
\]

- Higher variance â‡’ values further from mean.  
- Higher entropy â‡’ probabilities more uniform.

---

## 2ï¸âƒ£ Random-Variable Transformations
- Define a new variable: `Z = XÂ²`  
- **Law of the Unconscious Statistician (LOTUS):**

\[
E[g(X)] = \sum g(x) P(X=x)
\]

No need to derive PMF of Z; use Xâ€™s PMF directly.

### Expectation Properties
If a,b constants and X,Y RVs:

\[
E(aX)=aE(X),\quad E(X+Y)=E(X)+E(Y),\quad E(aX+bY)=aE(X)+bE(Y)
\]

âš ï¸ Not always: `E(XY) â‰  E(X)E(Y)` (unless independent).  
Also `E(XÂ²) â‰  [E(X)]Â²`.

### Variance Properties (Independent X,Y)
\[
Var(aX)=aÂ²Var(X),\quad Var(X+Y)=Var(X)+Var(Y)
\]

---

## 3ï¸âƒ£ Distribution Families
A *family* = set of distributions defined by its parameters.

Example: Binomial family has parameters n and p.  
Choosing specific n,p â†’ a unique distribution.

---

## 4ï¸âƒ£ Key Discrete Distribution Families

### **Bernoulli (p)**
Experiment = success/failure.

\[
P(X=x) = p^x (1-p)^{1-x},\quad xâˆˆ{0,1}
\]

E[X] = pâ€ƒVar[X] = p(1âˆ’p)

---

### **Binomial (n,p)**
n independent Bernoulli trials.

\[
P(X=x) = {n\choose x} p^x (1-p)^{n-x},\; x=0â€¦n
\]

E[X] = npâ€ƒVar[X] = np(1âˆ’p)

*Example:* P(win exactly 2 of 5) = C(5,2)(0.25)Â²(0.75)Â³ = 0.26

---

### **Geometric (p)**
### of failures before 1st success.

\[
P(X=x)=p(1-p)^x,\; x=0,1,2â€¦
\]
E[X]=(1âˆ’p)/pâ€ƒVar[X]=(1âˆ’p)/pÂ²

---

### **Negative Binomial (k,p)**
### of failures before k successes.

\[
P(X=x)={kâˆ’1+x\choose x} p^k(1âˆ’p)^x
\]
E[X]=k(1âˆ’p)/pâ€ƒVar[X]=k(1âˆ’p)/pÂ²  
Special case k=1 â†’ Geometric.

---

### **Poisson (Î»)**
### of events in fixed interval (time or space).

\[
P(X=x)=\frac{Î»^x e^{âˆ’Î»}}{x!},\; x=0,1,2â€¦
\]
E[X]=Î»â€ƒVar[X]=Î»  
Typical uses: # emails/day, # arrivals/hour, etc.

---

## 5ï¸âƒ£ Parameters & Parameterization
- **Parameter:** numeric value that defines distribution shape (e.g., p or Î»).  
- **Parameterization:** how those parameters identify a member within the family.  
- *Canonical parameterization* = most common form (e.g., Binomial â†’ n,p).

---

## ğŸ§  Key Takeaways
- Mean = expected value; variance = spread; entropy = uncertainty.  
- Transformations â†’ use LOTUS for E[g(X)].  
- Families group distributions by parameters.  
- Common families: Bernoulli, Binomial, Geometric, Negative Binomial, Poisson.  
- Poisson notable: mean = variance.

---

# ğŸ“Š DSCI 551 Lecture 3 â€” Joint Distributions & Dependence

A refresher on how multiple random variables interact through joint, marginal, and conditional distributions, and how dependence is measured.

---

## 1ï¸âƒ£ Joint Distributions

- A **joint distribution** gives the probability of every possible combination of two (or more) random variables.  
  Example: two fair coins (X = first coin, Y = second coin)

| X\Y | H  | T  |
|------|----|----|
| H | 0.25 | 0.25 |
| T | 0.25 | 0.25 |

Each cell â†’ `P(X=x, Y=y)`.

> For independent fair coins:  
> `P(X=H âˆ© Y=H) = P(X=H)Â·P(Y=H) = 0.5Â·0.5 = 0.25`.

The full table must sum to 1.

---

## 2ï¸âƒ£ Marginal Distributions

The probability distribution of one variable alone.  
Compute by **summing** over the other variable:

\[
P(X=x) = \sum_y P(X=x, Y=y)
\]
\[
P(Y=y) = \sum_x P(X=x, Y=y)
\]

> Only the *whole table* sums to 1 â€” not each row/column.

---

## 3ï¸âƒ£ Independence of Random Variables

Two RVs X, Y are independent iff

\[
P(X=x, Y=y) = P(X=x) Â· P(Y=y)\quad \forall x,y
\]

Equivalent properties:

- \(E[XY]=E[X]E[Y]\)  
- \(Var(X+Y)=Var(X)+Var(Y)\) when independent  

Independence means knowing one variable gives no information about the other.

---

## 4ï¸âƒ£ Dependence & Covariance

When X and Y are not independent, we can measure their dependence.

**Covariance**
\[
Cov(X,Y)=E[(Xâˆ’E[X])(Yâˆ’E[Y])]=E[XY]âˆ’E[X]E[Y]
\]

- \(Cov>0\) â‡’ as X increases, Y tends to increase.  
- \(Cov<0\) â‡’ as X increases, Y tends to decrease.  
- \(Cov=0\) â‡’ no *linear* relationship (but not necessarily independent).

**Example:**  
Ship length of stay (LOS) vs gang demand  
â†’ negative covariance â‰ˆ âˆ’0.74 â†’ longer stays â†’ fewer gangs requested.

---

## 5ï¸âƒ£ Correlation Coefficients

### **Pearson Correlation (Ï)**  
Standardizes covariance to (âˆ’1 â€¦ 1):

\[
Ï(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}
\]

- -1 â†’ perfect negative linear relation  
- 0 â†’ no linear relation (â‰  independence)  
- 1 â†’ perfect positive linear relation  
Invariant to scaling: Corr(10X, Y)=Corr(X,Y)

---

### **Kendallâ€™s Ï„ (Rank Correlation)**
For ordinal or non-linear relations.

\[
Ï„_K=\frac{\# concordant pairsâˆ’\# discordant pairs}{n(nâˆ’1)/2}
\]

- Ï„ â‰ˆ 1 â†’ strong positive association  
- Ï„ â‰ˆ 0 â†’ no monotonic relationship  
- Ï„ â‰ˆ âˆ’1 â†’ strong negative association

Captures *monotonic* (not just linear) dependence.

---

## 6ï¸âƒ£ Variance of a Sum

For any two RVs:

\[
Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)
\]

If independent â†’ Cov(X,Y)=0 â†’ \(Var(X+Y)=Var(X)+Var(Y)\).

---

## 7ï¸âƒ£ Quick Reference Formulas

| Concept | Formula | Notes |
|----------|----------|-------|
| Joint probability | \(P(X,Y)\) | table of combined outcomes |
| Marginal | \(\sum P(X,Y)\) | sum over other var |
| Independence | \(P(X,Y)=P(X)P(Y)\) | or \(E[XY]=E[X]E[Y]\) |
| Covariance | \(E[XY]âˆ’E[X]E[Y]\) | can be Â± |
| Pearson Ï | \(Cov(X,Y)/âˆš(Var(X)Var(Y))\) | linear dependence (âˆ’1â†’1) |
| Kendall Ï„ | rank-based measure | monotonic dependence |

---

## ğŸ§  Key Takeaways
- Marginals are derived by summing joint probabilities.  
- Independence simplifies joint â†’ product of marginals.  
- Covariance and correlation quantify dependence.  
- Pearson = linear relationship; Kendall Ï„ = monotonic relationship.  
- Variance of sum includes covariance term if variables dependent.

---

# ğŸ¯ DSCI 551 Lecture 4 â€” Conditional Probability & Independence

Clear, practical refresher on conditional probabilities, conditional expectations, and conditional independence.

---

## 1ï¸âƒ£ Univariate Conditional Probability

**Definition**

\[
P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0
\]

Interpretation â†’ event B becomes the new sample space.

---

### Example: Ship Length of Stay (L)

| L (days) | P(L) |
|-----------|------|
| 1 | 0.25 |
| 2 | 0.35 |
| 3 | 0.20 |
| 4 | 0.10 |
| 5 | 0.10 |

**Find:** distribution of L given L > 2

\[
P(L=l \mid L>2)=\frac{P(L=l)}{P(L>2)}
\]
\[
P(L>2)=0.2+0.1+0.1=0.4
\]

| L | P(L=l \| L>2) |
|---|----------------|
| 3 | 0.50 |
| 4 | 0.25 |
| 5 | 0.25 |

â†’ conditional distribution must still sum to 1 (renormalized).

---

## 2ï¸âƒ£ Conditional Probability Distributions (Multivariate)

For discrete random variables X and Y:

\[
P(X=x \mid Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)}
\]

A proper PMF:
\[
\sum_x P(X=x \mid Y=y) = 1
\]

---

### Example: Cargo Ships (L = length of stay, G = gangs)

Conditional on L = 1:

\[
P(G=g \mid L=1) = \frac{P(G=g, L=1)}{P(L=1)}
\]

| G | P(G=g \| L=1) |
|---|----------------|
| 1 | 0.0068 |
| 2 | 0.1701 |
| 3 | 0.4988 |
| 4 | 0.3242 |

âœ… Check: probabilities sum to 1.

---

### Conditional Expectation

For discrete X | Y=y:

\[
E(X \mid Y=y)=\sum_x x P(X=x\mid Y=y)
\]

**Example:**  
\(E(G)=2.3\);  
\(E(G \mid L=1)=3.14\) â‡’ expected # of gangs increases when stay = 1 day.

---

## 3ï¸âƒ£ Independence & Conditional Independence

### Independence
\[
P(X=x, Y=y)=P(X=x)\,P(Y=y)
\]
\[
P(Y=y\mid X=x)=P(Y=y)
\]

â†’ knowing X does not change belief about Y.

---

### Conditional Independence
X and Y are conditionally independent given Z if:
\[
P(X=x, Y=y \mid Z=z)=P(X=x \mid Z=z)\,P(Y=y \mid Z=z)
\]

---

#### Example: DSCI 551 Grades

- L = Lab grade (low/high)  
- Q = Quiz grade (low/high)  
- S = Statistics major (yes/no)

Without conditioning â†’ L and Q not independent.  
Given S = yes or S = no â†’ conditionally independent.

So:  
> Lab and quiz grades become independent *after conditioning* on whether the student is a Statistics major.

---

## 4ï¸âƒ£ Law of Total Expectation

A marginal mean can be reconstructed from conditional means:

\[
E[Y] = \sum_x E[Y\mid X=x]\,P(X=x)
\]
or equivalently  
\[
E[Y]=E_X[E[Y\mid X]]
\]

**Example (ships):**

| L | E(G | L) | P(L) | Product |
|---|-----------|-------|----------|
| 1 | 3.14 | 0.25 | 0.785 |
| 2 | 2.41 | 0.35 | 0.844 |
| 3 | 1.92 | 0.20 | 0.383 |
| 4 | 1.60 | 0.10 | 0.160 |
| 5 | 1.27 | 0.10 | 0.127 |

\[
E(G)=\sum E(G\mid L)\,P(L)=2.3
\]

---

## 5ï¸âƒ£ Key Takeaways

- **Conditional probability:** restrict sample space â†’ renormalize.  
- **Conditional distribution:** table or formula form; sums to 1.  
- **Conditional expectation:** expected value under new condition.  
- **Independence:** joint = product of marginals.  
- **Conditional independence:** joint conditional = product of conditionals.  
- **Law of Total Expectation:** marginal = weighted average of conditionals.

---
# ğŸ“ˆ DSCI 551 Lecture 5 â€” Continuous Distributions

A concise refresher on continuous random variables, probability density functions (PDFs), their properties, and key continuous distribution tools.

---

## 1ï¸âƒ£ Continuous Random Variables
- Outcomes are **uncountably infinite** (e.g., temperature, water level, stock price).  
- Treated as continuous when neighboring valuesâ€™ differences are negligible.  
- Measurement precision is limited by instruments.

ğŸ§© Rule of thumb: treat a variable as continuous if â€œÂ± 0.01â€ changes donâ€™t matter.

---

## 2ï¸âƒ£ Probability Density Function (PDF)

### Physical analogy â€” density
Think of **mass density**: weight per unit length (tree example).  
The *total mass* = âˆ« density Ã— thickness.  
Analogously, total probability = âˆ« f(x) dx = 1.

### Key properties
| Property | Discrete (PMF) | Continuous (PDF) |
|-----------|----------------|------------------|
| Symbol | p(x)=P(X=x) | f(x) |
| Probability calc | directly P(X=x) | integrate over range |
| Normalization | Î£ p(x)=1 | âˆ« f(x) dx=1 |

### Probability of an interval
\[
P(a â‰¤ X â‰¤ b) = \int_a^b f_X(x)\,dx
\]
Probability of an exact value is **zero**.  
ğŸ‘‰ We only compute probabilities **over intervals**.

### Units
PDF units = 1 / (unit of x)  
E.g. height â†’ mâ»Â¹

---

### Example: Low-Purity Octane
\[
f_X(x)=
\begin{cases}
2x, & 0â‰¤xâ‰¤1\\
0, & \text{elsewhere}
\end{cases}
\]

- Valid PDF since âˆ«â‚€Â¹ 2x dx = 1.  
- \(P(X=0.25)=0\) (single value).  
- \(P(X<0.5)=\intâ‚€^{0.5}2x\,dx=0.25.\)

---

## 3ï¸âƒ£ Distribution Properties

### Mean & Variance
\[
E[X]=\int x f_X(x)\,dx,\qquad
Var[X]=E[(Xâˆ’E[X])Â²]=E[XÂ²]âˆ’(E[X])Â²
\]

Example: for f(x)=2x on [0,1]:
- \(E[X]=2/3 â‰ˆ 0.67\)
- \(Var[X]=1/18 â‰ˆ 0.056\)

---

### Mode
\[
Mode(X)=\arg\max_x f_X(x)
\]
Highest density point; less useful for continuous X.

For f(x)=2x on [0,1]: Mode = 1.

---

### Entropy (â€œdifferential entropyâ€)
\[
H(X)=âˆ’\int f(x)\log f(x)\,dx
\]
Can be positive, negative, or âˆ.  
Example: H â‰ˆ âˆ’0.19 for f(x)=2x on [0,1].

---

### Median and Quantiles
- **Median M:** P(X â‰¤ M)=0.5  
  â†’ for f(x)=2x: MÂ²=0.5 â†’ M = 0.7071  
- **Quantile Q(p):** P(X â‰¤ Q(p))=p  
  â†’ Q(0.25) = âˆš0.25 = 0.5  

---

### Prediction Intervals via Quantiles
For central p probability:
\[
P(Q_{(1âˆ’p)/2} < X < Q_{(1+p)/2}) = p
\]
Example (90% interval for f(x)=2x): Q(0.05)=0.2236, Q(0.95)=0.9747.

---

### Skewness
Measures asymmetry:
\[
Skew(X)=E\!\left[\!\left(\frac{Xâˆ’Î¼}{Ïƒ}\right)^3\!\right]
\]
- = 0 â†’ symmetric  
- > 0 â†’ right-skewed  
- < 0 â†’ left-skewed

---

## 4ï¸âƒ£ Alternative Distribution Representations

| Representation | Definition | Notes |
|----------------|-------------|-------|
| **CDF F(x)** | P(X â‰¤ x)=âˆ«_{âˆ’âˆ}^x f(t) dt | Non-decreasing, 0â†’1 |
| **Survival S(x)** | P(X > x)=1âˆ’F(x) | â€œflipâ€ of CDF |
| **Quantile Q(p)** | Fâ»Â¹(p) | maps prob â†’ value |
| **PDF â†” CDF** | f(x)=dF/dx | CDF flat â†’ PDF 0 |

Valid CDF conditions:
1ï¸âƒ£ Non-decreasing  
2ï¸âƒ£ 0 â‰¤ F(x) â‰¤ 1  
3ï¸âƒ£ F(âˆ’âˆ)=0, F(âˆ)=1

**Using CDF for probabilities:**
\[
P(aâ‰¤Xâ‰¤b)=F(b)âˆ’F(a)
\]

---

## 5ï¸âƒ£ Exponential Distribution

Models time between Poisson events (wait time until next event).

### Parameterizations
\[
Xâˆ¼Exp(Î») \quad\text{or}\quad Xâˆ¼Exp(Î²=1/Î»)
\]
- Î» = rate (events per unit time)  
- Î² = mean wait time

### PDF & Properties
\[
f(x)=Î»e^{âˆ’Î»x},\quad xâ‰¥0
\]
\[
E[X]=1/Î»,\quad Var[X]=1/Î»Â²
\]
Memoryless property:
\[
P(X>t+s\mid X>s)=P(X>t)
\]

---

### R functions (in practice)
| Function | Purpose |
|-----------|----------|
| `dexp(x, rate)` | density f(x) |
| `pexp(q, rate)` | CDF F(x) |
| `qexp(p, rate)` | quantile Q(p) |
| `rexp(n, rate)` | random draws |

---

## ğŸ§  Key Takeaways
- Continuous RV â†’ uncountable outcomes.  
- Probabilities = areas under PDF; P(X=x)=0.  
- Mean = expected value via integral.  
- Quantiles generalize median; enable prediction intervals.  
- CDF, Survival, Quantile functions all represent same distribution.  
- Exponential distribution = Poisson inter-arrival times; memoryless.

---






