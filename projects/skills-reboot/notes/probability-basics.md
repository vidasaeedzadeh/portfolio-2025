# ğŸ² DSCI 551 â€“ Lecture 1 Notes : Depicting Uncertainty

## 1ï¸âƒ£ Core Concepts of Probability
- **Experiment** â†’ an action with uncertain outcome (coin toss, die roll).  
- **Sample space (S)** â†’ all possible outcomes.  
  - Example: `S = {Heads, Tails}`  
- **Event (E)** â†’ subset of S (e.g. `E = {Heads}`).

**Empirical probability**
\[
P(E) = \frac{\text{# times E occurs}}{\text{# total experiments}}
\]
approaches the true value as n â†’ âˆ.

---

## 2ï¸âƒ£ Axioms & Laws
1. **Axiom 1 (Sample space)** â†’ `P(S)=1`  
2. **Axiom 2 (Mutually exclusive events)** â†’ `P(AâˆªB)=P(A)+P(B)` if Aâˆ©B=âˆ…  
3. **Complement rule** â†’ `P(Ac)=1âˆ’P(A)`  
4. **Law of Total Probability** â†’ probability of E = sum of probabilities of its disjoint parts  
5. **Inclusionâ€“Exclusion Principle**


P(A âˆª B) = P(A) + P(B) âˆ’ P(A âˆ© B)

extended to 3+ events by alternating + / â€“ terms.  
6. **Independence** â†’ `P(A âˆ© B)=P(A)Â·P(B)`

---

## 3ï¸âƒ£ Odds â†” Probability
\[
o = \frac{p}{1-p}, \quad p = \frac{o}{o+1}
\]
Example: p = 0.8 â‡’ o = 4 : 1 (4 wins per loss)

---

## 4ï¸âƒ£ Random Variables (RV)
- **Discrete RV:** countable outcomes â†’ Probability Mass Function (PMF) `P(X=x)`
- **Continuous RV:** uncountable outcomes â†’ Probability Density Function (PDF) `fX(x)`
- Example: X = # heads in 10 coin tosses.

---

## 5ï¸âƒ£ Measures of Central Tendency & Uncertainty

| Measure | Meaning | Formula |
|----------|----------|----------|
| **Mean (E[X])** | expected/typical value | `Î£ x P(X=x)` or `âˆ« x f(x) dx` |
| **Variance (Var[X])** | spread/uncertainty | `E[(Xâˆ’E[X])Â²] = E[XÂ²]âˆ’E[X]Â²` |
| **Std Dev (Ïƒ)** | sqrt of variance | `Ïƒ = âˆšVar[X]` |
| **Mode** | most probable outcome | argmax P(X=x) |
| **Entropy H(X)** | randomness in bits/nats | `âˆ’Î£ P(X=x) log P(X=x)` |

---

## 6ï¸âƒ£ Examples to Remember
- **Mario Kart items** as a discrete distribution: probs sum to 1.  
- **Complement example:** P(not Coin) = 1 âˆ’ 0.75 = 0.25.  
- **Inclusionâ€“Exclusion example:** Explosion âˆ¨ Blue-shell item = 0.08.  
- **Entropy example:** H â‰ˆ 0.87 â†’ some randomness but not uniform.

---

## 7ï¸âƒ£ Key Formulas Summary
```text
P(Ac) = 1 âˆ’ P(A)
P(A âˆª B) = P(A) + P(B) âˆ’ P(A âˆ© B)
P(A âˆ© B) = P(A)Â·P(B)   # if independent
Var(X) = E[XÂ²] âˆ’ (E[X])Â²
H(X) = âˆ’Î£ P(x)Â·log P(x)
```
---

# ğŸ¯ DSCI 551 Lecture 2 â€“ Parametric Families

Refresher on probability distributions, random-variable transformations, and the most common discrete distribution families.

---

## 1ï¸âƒ£ Review â€” Properties of Distributions
Example PMF (Number of crabs per nest):

| x | P(X = x) |
|---|-----------|
| 0 | 0.4 |
| 1 | 0.1 |
| 2 | 0.1 |
| 3 | 0.4 |

**Mean**

\[
E[X] = \sum x P(X=x) = 1.5
\]

**Variance**

\[
Var(X) = E[(Xâˆ’E[X])Â²] = 1.85
\]

**Mode** â†’ values with max probability â†’ 0 and 3  
**Entropy**

\[
H(X) = âˆ’ \sum P(X=x) log P(X=x) = 1.19
\]

- Higher variance â‡’ values further from mean.  
- Higher entropy â‡’ probabilities more uniform.

---

## 2ï¸âƒ£ Random-Variable Transformations
- Define a new variable: `Z = XÂ²`  
- **Law of the Unconscious Statistician (LOTUS):**

\[
E[g(X)] = \sum g(x) P(X=x)
\]

No need to derive PMF of Z; use Xâ€™s PMF directly.

### Expectation Properties
If a,b constants and X,Y RVs:

\[
E(aX)=aE(X),\quad E(X+Y)=E(X)+E(Y),\quad E(aX+bY)=aE(X)+bE(Y)
\]

âš ï¸ Not always: `E(XY) â‰  E(X)E(Y)` (unless independent).  
Also `E(XÂ²) â‰  [E(X)]Â²`.

### Variance Properties (Independent X,Y)
\[
Var(aX)=aÂ²Var(X),\quad Var(X+Y)=Var(X)+Var(Y)
\]

---

## 3ï¸âƒ£ Distribution Families
A *family* = set of distributions defined by its parameters.

Example: Binomial family has parameters n and p.  
Choosing specific n,p â†’ a unique distribution.

---

## 4ï¸âƒ£ Key Discrete Distribution Families

### **Bernoulli (p)**
Experiment = success/failure.

\[
P(X=x) = p^x (1-p)^{1-x},\quad xâˆˆ{0,1}
\]

E[X] = pâ€ƒVar[X] = p(1âˆ’p)

---

### **Binomial (n,p)**
n independent Bernoulli trials.

\[
P(X=x) = {n\choose x} p^x (1-p)^{n-x},\; x=0â€¦n
\]

E[X] = npâ€ƒVar[X] = np(1âˆ’p)

*Example:* P(win exactly 2 of 5) = C(5,2)(0.25)Â²(0.75)Â³ = 0.26

---

### **Geometric (p)**
### of failures before 1st success.

\[
P(X=x)=p(1-p)^x,\; x=0,1,2â€¦
\]
E[X]=(1âˆ’p)/pâ€ƒVar[X]=(1âˆ’p)/pÂ²

---

### **Negative Binomial (k,p)**
### of failures before k successes.

\[
P(X=x)={kâˆ’1+x\choose x} p^k(1âˆ’p)^x
\]
E[X]=k(1âˆ’p)/pâ€ƒVar[X]=k(1âˆ’p)/pÂ²  
Special case k=1 â†’ Geometric.

---

### **Poisson (Î»)**
### of events in fixed interval (time or space).

\[
P(X=x)=\frac{Î»^x e^{âˆ’Î»}}{x!},\; x=0,1,2â€¦
\]
E[X]=Î»â€ƒVar[X]=Î»  
Typical uses: # emails/day, # arrivals/hour, etc.

---

## 5ï¸âƒ£ Parameters & Parameterization
- **Parameter:** numeric value that defines distribution shape (e.g., p or Î»).  
- **Parameterization:** how those parameters identify a member within the family.  
- *Canonical parameterization* = most common form (e.g., Binomial â†’ n,p).

---

## ğŸ§  Key Takeaways
- Mean = expected value; variance = spread; entropy = uncertainty.  
- Transformations â†’ use LOTUS for E[g(X)].  
- Families group distributions by parameters.  
- Common families: Bernoulli, Binomial, Geometric, Negative Binomial, Poisson.  
- Poisson notable: mean = variance.

---


